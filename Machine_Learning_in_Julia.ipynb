{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JuliaDB"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import JuliaDB\n",
    "using JuliaDB: ML\n",
    "const Jdb = JuliaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table with 150 rows, 5 columns:\n",
       "SepalLength  SepalWidth  PetalLength  PetalWidth  Species\n",
       "─────────────────────────────────────────────────────────────\n",
       "5.1          3.5         1.4          0.2         \"setosa\"\n",
       "4.9          3.0         1.4          0.2         \"setosa\"\n",
       "4.7          3.2         1.3          0.2         \"setosa\"\n",
       "4.6          3.1         1.5          0.2         \"setosa\"\n",
       "5.0          3.6         1.4          0.2         \"setosa\"\n",
       "5.4          3.9         1.7          0.4         \"setosa\"\n",
       "4.6          3.4         1.4          0.3         \"setosa\"\n",
       "5.0          3.4         1.5          0.2         \"setosa\"\n",
       "4.4          2.9         1.4          0.2         \"setosa\"\n",
       "4.9          3.1         1.5          0.1         \"setosa\"\n",
       "5.4          3.7         1.5          0.2         \"setosa\"\n",
       "4.8          3.4         1.6          0.2         \"setosa\"\n",
       "⋮\n",
       "6.9          3.1         5.4          2.1         \"virginica\"\n",
       "6.7          3.1         5.6          2.4         \"virginica\"\n",
       "6.9          3.1         5.1          2.3         \"virginica\"\n",
       "5.8          2.7         5.1          1.9         \"virginica\"\n",
       "6.8          3.2         5.9          2.3         \"virginica\"\n",
       "6.7          3.3         5.7          2.5         \"virginica\"\n",
       "6.7          3.0         5.2          2.3         \"virginica\"\n",
       "6.3          2.5         5.0          1.9         \"virginica\"\n",
       "6.5          3.0         5.2          2.0         \"virginica\"\n",
       "6.2          3.4         5.4          2.3         \"virginica\"\n",
       "5.9          3.0         5.1          1.8         \"virginica\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = Jdb.loadtable(\"iris.csv\",escapechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 5 entries:\n",
       "  :SepalLength => Continous(μ=5.843333333333333, σ=0.8280661279778638)\n",
       "  :SepalWidth  => Continous(μ=3.0573333333333315, σ=0.43586628493669777)\n",
       "  :PetalWidth  => Continous(μ=1.1993333333333336, σ=0.7622376689603466)\n",
       "  :PetalLength => Continous(μ=3.758, σ=1.7652982332594667)\n",
       "  :Species     => nothing"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML.schema(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 5 entries:\n",
       "  :SepalLength => Continous(μ=5.843333333333333, σ=0.8280661279778638)\n",
       "  :SepalWidth  => Continous(μ=3.0573333333333315, σ=0.43586628493669777)\n",
       "  :PetalWidth  => Continous(μ=1.1993333333333336, σ=0.7622376689603466)\n",
       "  :PetalLength => Continous(μ=3.758, σ=1.7652982332594667)\n",
       "  :Species     => Categorical([\"setosa\", \"versicolor\", \"virginica\"])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_schema = ML.schema(iris,hints = Dict(\n",
    "        :Species => ML.Categorical\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: In `filter(f, dict)`, `f` is now passed a single pair instead of two arguments.\n",
      "│   caller = splitschema(::Dict{Symbol,Any}, ::Symbol) at ml.jl:155\n",
      "└ @ JuliaDB.ML /Users/ppalmes/.julia/packages/JuliaDB/jDAlJ/src/ml.jl:155\n",
      "┌ Warning: In `filter(f, dict)`, `f` is now passed a single pair instead of two arguments.\n",
      "│   caller = splitschema(::Dict{Symbol,Any}, ::Symbol) at ml.jl:155\n",
      "└ @ JuliaDB.ML /Users/ppalmes/.julia/packages/JuliaDB/jDAlJ/src/ml.jl:155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dict{Symbol,Any}(:SepalLength=>Continous(μ=5.843333333333333, σ=0.8280661279778638),:SepalWidth=>Continous(μ=3.0573333333333315, σ=0.43586628493669777),:PetalWidth=>Continous(μ=1.1993333333333336, σ=0.7622376689603466),:PetalLength=>Continous(μ=3.758, σ=1.7652982332594667)), Dict{Symbol,Any}(:Species=>Categorical([\"setosa\", \"versicolor\", \"virginica\"])))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_schema, output_schema = ML.splitschema(iris_schema,:Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×150 LinearAlgebra.Adjoint{Float32,Array{Float32,2}}:\n",
       " -0.897674  -1.1392    -1.38073   …   0.793012  0.430722   0.0684325\n",
       "  1.0156    -0.131539   0.327318     -0.131539  0.786174  -0.131539 \n",
       " -1.31105   -1.31105   -1.31105       1.05042   1.44399    0.788031 \n",
       " -1.33575   -1.33575   -1.3924        0.816859  0.930154   0.760211 "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = ML.featuremat(input_schema,iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×150 LinearAlgebra.Adjoint{Float32,Array{Float32,2}}:\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = ML.featuremat(output_schema,iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitionTrainTest (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "function partitionTrainTest(input,output, at = 0.8)\n",
    "    @assert size(input,2) == size(output,2)\n",
    "    n = size(input,2)\n",
    "    idx = shuffle(1:n)\n",
    "    train_idx = view(idx, 1:floor(Int, at*n))\n",
    "    test_idx = view(idx, (floor(Int, at*n)+1):n)\n",
    "    return input[:,train_idx], output[:,train_idx], input[:,test_idx],output[:,test_idx]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[1.03454 -1.01844 … -0.414621 0.309959; 0.0978893 0.786174 … -1.04925 -0.131539; 0.26326 -1.31105 … 0.000874618 0.788031; 0.363678 -1.2791 … 0.363678 0.646916], Float32[0.0 1.0 … 0.0 0.0; 1.0 0.0 … 1.0 0.0; 0.0 0.0 … 0.0 1.0], Float32[2.24217 -0.173094 … 2.24217 1.39683; -0.590395 -0.360967 … 1.70389 0.327318; 1.05042 0.132067 … 1.3128 0.26326; 1.66657 0.250383 … 1.66657 0.533621], Float32[0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 1.0; 1.0 0.0 … 1.0 0.0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input,train_output,test_input,test_output = partitionTrainTest(input,output,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flux"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Flux\n",
    "const Fx = Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: ADAM(params) is deprecated; use ADAM(η::Float64) instead\n",
      "│   caller = top-level scope at In[77]:8\n",
      "└ @ Core In[77]:8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "#24 (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Fx.Chain(\n",
    "    Fx.Dense(ML.width(input_schema),5,Fx.relu),\n",
    "    Fx.Dense(5,ML.width(output_schema)),\n",
    "    Fx.softmax\n",
    ")\n",
    "\n",
    "loss(x, y) = Flux.mse(model(x), y)\n",
    "#loss(x,y) = Fx.crossentropy(model(x),y)\n",
    "opt = Flux.ADAM(Flux.params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(train_input, train_output)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(first(data)...) = 0.24238965f0 (tracked)\n",
      "loss(first(data)...) = 0.24238965f0 (tracked)\n",
      "loss(first(data)...) = 0.24238965f0 (tracked)\n"
     ]
    }
   ],
   "source": [
    "evalcb = Fx.throttle(() -> @show(loss(first(data)...)), 0.2);\n",
    "for i = 1:1000\n",
    "  Fx.train!(loss, data, opt, cb = evalcb)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 15.555555555555555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = top-level scope at In[82]:2\n",
      "└ @ Core In[82]:2\n",
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = top-level scope at In[82]:4\n",
      "└ @ Core In[82]:4\n"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "yhat=Fx.argmax(model(test_input))\n",
    "y=Fx.argmax(test_output)\n",
    "fx_accuracy = mean(y .== yhat) * 100\n",
    "print(\"accuracy: \",fx_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TensorFlow\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_test_set (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "import Random\n",
    "\n",
    "mutable struct DataLoader\n",
    "    cur_id::Int\n",
    "    order::Vector{Int}\n",
    "end\n",
    "\n",
    "DataLoader() = DataLoader(1, Random.shuffle(1:60000))\n",
    "\n",
    "function next_batch(loader::DataLoader, batch_size)\n",
    "    x = zeros(Float32, batch_size, 784)\n",
    "    y = zeros(Float32, batch_size, 10)\n",
    "    for i in 1:batch_size\n",
    "        data, label = MLDatasets.MNIST.traindata(loader.order[loader.cur_id])\n",
    "        x[i, :] = reshape(data, (28*28))\n",
    "        y[i, Int(label)+1] = 1.0\n",
    "        loader.cur_id += 1\n",
    "        if loader.cur_id > 60000\n",
    "            loader.cur_id = 1\n",
    "        end\n",
    "    end\n",
    "    x, y\n",
    "end\n",
    "\n",
    "function load_test_set(N=10000)\n",
    "    x = zeros(Float32, N, 784)\n",
    "    y = zeros(Float32, N, 10)\n",
    "    for i in 1:N\n",
    "        data, label = MLDatasets.MNIST.testdata(i)\n",
    "        x[i, :] = reshape(data, (28*28))\n",
    "        y[i, Int(label)+1] = 1.0\n",
    "    end\n",
    "    x,y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-26 22:56:05.828595: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Serialization.__deserialized_types__, Symbol(\"##7#8\")))() at TensorFlow.jl:189\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/TensorFlow.jl:189\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = make_py_graph(::Array{UInt8,1}) at py.jl:51\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:51\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = make_py_graph(::Array{UInt8,1}) at py.jl:52\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:52\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = py_with(::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}}, ::PyObject) at py.jl:19\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:19\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}})() at py.jl:54\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:54\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}})() at py.jl:54\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:54\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}})() at py.jl:54\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:54\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}})() at py.jl:55\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:55\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}})() at py.jl:42\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:42\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}})() at py.jl:42\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:42\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = py_with(::getfield(Main, Symbol(\"##3#4\")){Array{UInt8,1}}, ::PyObject) at py.jl:21\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:21\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = (::getfield(Main, Symbol(\"#to_py_node#5\")){PyObject})(::Tuple{String,Int64}) at py.jl:77\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:77\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = py_gradients(::Array{UInt8,1}, ::Array{Tuple{String,Int64},1}, ::Tuple{String,Int64}, ::Nothing) at py.jl:42\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:42\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = py_gradients(::Array{UInt8,1}, ::Array{Tuple{String,Int64},1}, ::Tuple{String,Int64}, ::Nothing) at py.jl:85\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:85\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = py_gradients(::Array{UInt8,1}, ::Array{Tuple{String,Int64},1}, ::Tuple{String,Int64}, ::Nothing) at py.jl:93\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:93\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = py_gradients(::Array{UInt8,1}, ::Array{Tuple{String,Int64},1}, ::Tuple{String,Int64}, ::Nothing) at py.jl:95\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:95\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = to_protos(::PyObject) at py.jl:63\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:63\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = to_protos(::PyObject) at py.jl:68\n",
      "└ @ Main ~/.julia/packages/TensorFlow/q9pY2/src/py.jl:68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Tensor reduce_3:1 shape=unknown dtype=Float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "\n",
    "sess = Session(Graph())\n",
    "\n",
    "x = placeholder(Float32)\n",
    "y_ = placeholder(Float32)\n",
    "\n",
    "W = Variable(zeros(Float32, 784, 10))\n",
    "b = Variable(zeros(Float32, 10))\n",
    "\n",
    "run(sess, global_variables_initializer())\n",
    "\n",
    "y = nn.softmax(x*W + b)\n",
    "\n",
    "cross_entropy = reduce_mean(-reduce_sum(y_ .* log(y), axis=[2]))\n",
    "train_step = train.minimize(train.GradientDescentOptimizer(.00001), cross_entropy)\n",
    "\n",
    "correct_prediction = argmax(y, 2) .== argmax(y_, 2)\n",
    "accuracy=reduce_mean(cast(correct_prediction, Float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:10\n",
    "    batch = next_batch(loader, 100)\n",
    "    run(sess, train_step, Dict(x=>batch[1], y_=>batch[2]))\n",
    "end\n",
    "\n",
    "testx, testy = load_test_set()\n",
    "\n",
    "println(run(sess, accuracy, Dict(x=>testx, y_=>testy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1192\n"
     ]
    }
   ],
   "source": [
    "import Knet\n",
    "using Flux: argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Array{Float32,2},1}:\n",
       " [0.0743644 -0.0904959 -0.0999296 -0.0962196; 0.0756233 0.1305 0.0355859 -0.147267; … ; 0.0670305 -0.235665 -0.0431451 0.0317444; 0.107203 0.0196627 -0.09373 -0.0625429]\n",
       " [0.0; 0.0; … ; 0.0; 0.0]                                                                                                                                                \n",
       " [-0.089309 0.0603609 … -0.116096 -0.178503; -0.0333412 -0.0582998 … -0.00667422 -0.0340108; 0.092459 0.0448854 … -0.091398 0.156921]                                    \n",
       " [0.0; 0.0; 0.0]                                                                                                                                                         "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Atype = Knet.gpu() >= 0 ? KnetArray{Float32} : Array{Float32}\n",
    "# setup weight parameters\n",
    "# iris data: input=>4, hidden=>5, output=>3,\n",
    "wmlp=map(Atype, [ 0.1*randn(5,4), zeros(5,1),\n",
    "                  0.1*randn(3,5),  zeros(3,1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "knetpredict (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function knetpredict(w,x)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*Knet.mat(x) .+ w[i+1]\n",
    "        if i<length(w)-1\n",
    "            x = max.(0,x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD training loop\n",
    "function train!(w, data; lr=.1)\n",
    "    for (x,y) in data\n",
    "        dw = lossgradient(w, x, y)\n",
    "        for i in 1:length(w)\n",
    "            w[i] -= lr * dw[i]\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(w,x,ygold) = Knet.nll(knetpredict(w,x),ygold); # nll is negative log likelihood\n",
    "lossgradient = Knet.grad(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = top-level scope at In[16]:2\n",
      "└ @ Core In[16]:2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.091057 seconds (8.53 M allocations: 450.637 MiB, 6.77% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000-element Array{Float32,1}:\n",
       " 1.0958363  \n",
       " 1.0919127  \n",
       " 1.0886558  \n",
       " 1.0855064  \n",
       " 1.0824982  \n",
       " 1.0794066  \n",
       " 1.0761452  \n",
       " 1.072549   \n",
       " 1.0686688  \n",
       " 1.064456   \n",
       " 1.059736   \n",
       " 1.0545866  \n",
       " 1.0489719  \n",
       " ⋮          \n",
       " 0.054413546\n",
       " 0.054396864\n",
       " 0.054380227\n",
       " 0.05436361 \n",
       " 0.054347042\n",
       " 0.054330528\n",
       " 0.054314036\n",
       " 0.054297596\n",
       " 0.054281197\n",
       " 0.05426483 \n",
       " 0.05424852 \n",
       " 0.05423224 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=train_input\n",
    "ygold=argmax(train_output)\n",
    "\n",
    "loss(wmlp,x,ygold)\n",
    "\n",
    "@time weights = [ copy(train!(wmlp, [(x, ygold)])) for epoch=1:1000 ]\n",
    "\n",
    "losses = [ loss(w,x,ygold) for w in weights ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses:\n",
      "====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11-element Array{Float32,1}:\n",
       " 0.054396864\n",
       " 0.054380227\n",
       " 0.05436361 \n",
       " 0.054347042\n",
       " 0.054330528\n",
       " 0.054314036\n",
       " 0.054297596\n",
       " 0.054281197\n",
       " 0.05426483 \n",
       " 0.05424852 \n",
       " 0.05423224 "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"losses:\")\n",
    "println(\"====\")\n",
    "[x for x in losses[end-10:end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip3400\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip3400)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip3401\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip3400)\" points=\"\n",
       "224.386,1440.48 2321.26,1440.48 2321.26,47.2441 224.386,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip3402\">\n",
       "    <rect x=\"224\" y=\"47\" width=\"2098\" height=\"1394\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  281.751,1440.48 281.751,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  776.792,1440.48 776.792,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1271.83,1440.48 1271.83,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1766.87,1440.48 1766.87,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1217.11 2321.26,1217.11 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,964.735 2321.26,964.735 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,712.36 2321.26,712.36 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,459.984 2321.26,459.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,207.609 2321.26,207.609 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 224.386,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  281.751,1440.48 281.751,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  776.792,1440.48 776.792,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1271.83,1440.48 1271.83,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1766.87,1440.48 1766.87,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1217.11 255.839,1217.11 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,964.735 255.839,964.735 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,712.36 255.839,712.36 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,459.984 255.839,459.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,207.609 255.839,207.609 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 281.751, 1494.48)\" x=\"281.751\" y=\"1494.48\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 776.792, 1494.48)\" x=\"776.792\" y=\"1494.48\">250</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1271.83, 1494.48)\" x=\"1271.83\" y=\"1494.48\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1766.87, 1494.48)\" x=\"1766.87\" y=\"1494.48\">750</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2261.91, 1494.48)\" x=\"2261.91\" y=\"1494.48\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1234.61)\" x=\"200.386\" y=\"1234.61\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 982.235)\" x=\"200.386\" y=\"982.235\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 729.86)\" x=\"200.386\" y=\"729.86\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 477.484)\" x=\"200.386\" y=\"477.484\">0.8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 225.109)\" x=\"200.386\" y=\"225.109\">1.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 1590.4)\" x=\"1272.82\" y=\"1590.4\">epochs</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 743.863)\" x=\"57.6\" y=\"743.863\">Loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip3402)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,86.6754 285.712,91.6264 287.692,95.7362 289.672,99.7104 291.652,103.506 293.632,107.408 295.613,111.523 297.593,116.061 299.573,120.957 301.553,126.273 \n",
       "  303.533,132.229 305.513,138.727 307.494,145.812 309.474,153.626 311.454,162.286 313.434,171.882 315.414,182.432 317.394,193.996 319.375,206.559 321.355,220.145 \n",
       "  323.335,234.792 325.315,250.516 327.295,267.312 329.275,285.141 331.256,303.914 333.236,323.53 335.216,343.877 337.196,364.803 339.176,386.127 341.156,407.717 \n",
       "  343.137,429.407 345.117,451.026 347.097,472.417 349.077,493.462 351.057,514.046 353.037,534.023 355.018,553.387 356.998,572.108 358.978,590.158 360.958,607.542 \n",
       "  362.938,624.254 364.918,640.314 366.898,655.74 368.879,670.562 370.859,684.804 372.839,698.476 374.819,711.606 376.799,724.222 378.779,736.338 380.76,747.987 \n",
       "  382.74,759.175 384.72,769.923 386.7,780.254 388.68,790.189 390.66,799.743 392.641,808.943 394.621,817.807 396.601,826.355 398.581,834.604 400.561,842.569 \n",
       "  402.541,850.268 404.522,857.717 406.502,864.929 408.482,871.921 410.462,878.704 412.442,885.29 414.422,891.692 416.403,897.914 418.383,903.972 420.363,909.875 \n",
       "  422.343,915.631 424.323,921.245 426.303,926.725 428.284,932.076 430.264,937.304 432.244,942.418 434.224,947.424 436.204,952.327 438.184,957.135 440.165,961.854 \n",
       "  442.145,966.478 444.125,971.014 446.105,975.465 448.085,979.833 450.065,984.12 452.045,988.322 454.026,992.441 456.006,996.487 457.986,1000.46 459.966,1004.37 \n",
       "  461.946,1008.2 463.926,1011.98 465.907,1015.68 467.887,1019.33 469.867,1022.91 471.847,1026.44 473.827,1029.9 475.807,1033.31 477.788,1036.66 479.768,1039.95 \n",
       "  481.748,1043.19 483.728,1046.37 485.708,1049.5 487.688,1052.59 489.669,1055.62 491.649,1058.6 493.629,1061.54 495.609,1064.43 497.589,1067.28 499.569,1070.08 \n",
       "  501.55,1072.84 503.53,1075.56 505.51,1078.24 507.49,1080.87 509.47,1083.47 511.45,1086.03 513.431,1088.55 515.411,1091.04 517.391,1093.49 519.371,1095.9 \n",
       "  521.351,1098.28 523.331,1100.62 525.312,1102.94 527.292,1105.22 529.272,1107.47 531.252,1109.69 533.232,1111.89 535.212,1114.05 537.192,1116.19 539.173,1118.3 \n",
       "  541.153,1120.38 543.133,1122.44 545.113,1124.47 547.093,1126.47 549.073,1128.45 551.054,1130.41 553.034,1132.34 555.014,1134.25 556.994,1136.14 558.974,1138.02 \n",
       "  560.954,1139.87 562.935,1141.7 564.915,1143.51 566.895,1145.3 568.875,1147.07 570.855,1148.83 572.835,1150.56 574.816,1152.29 576.796,1153.99 578.776,1155.68 \n",
       "  580.756,1157.36 582.736,1159.02 584.716,1160.66 586.697,1162.29 588.677,1163.91 590.657,1165.52 592.637,1167.11 594.617,1168.69 596.597,1170.26 598.578,1171.81 \n",
       "  600.558,1173.36 602.538,1174.89 604.518,1176.41 606.498,1177.92 608.478,1179.42 610.459,1180.91 612.439,1182.38 614.419,1183.85 616.399,1185.31 618.379,1186.75 \n",
       "  620.359,1188.19 622.339,1189.62 624.32,1191.04 626.3,1192.44 628.28,1193.84 630.26,1195.23 632.24,1196.61 634.22,1197.99 636.201,1199.35 638.181,1200.71 \n",
       "  640.161,1202.06 642.141,1203.4 644.121,1204.73 646.101,1206.05 648.082,1207.37 650.062,1208.68 652.042,1209.98 654.022,1211.27 656.002,1212.56 657.982,1213.83 \n",
       "  659.963,1215.1 661.943,1216.36 663.923,1217.62 665.903,1218.87 667.883,1220.11 669.863,1221.34 671.844,1222.57 673.824,1223.79 675.804,1225 677.784,1226.2 \n",
       "  679.764,1227.4 681.744,1228.6 683.725,1229.78 685.705,1230.96 687.685,1232.13 689.665,1233.3 691.645,1234.46 693.625,1235.61 695.606,1236.75 697.586,1237.89 \n",
       "  699.566,1239.03 701.546,1240.15 703.526,1241.27 705.506,1242.39 707.487,1243.49 709.467,1244.59 711.447,1245.68 713.427,1246.77 715.407,1247.85 717.387,1248.93 \n",
       "  719.367,1249.99 721.348,1251.06 723.328,1252.11 725.308,1253.16 727.288,1254.2 729.268,1255.23 731.248,1256.26 733.229,1257.28 735.209,1258.29 737.189,1259.3 \n",
       "  739.169,1260.3 741.149,1261.29 743.129,1262.28 745.11,1263.26 747.09,1264.24 749.07,1265.2 751.05,1266.17 753.03,1267.12 755.01,1268.07 756.991,1269.01 \n",
       "  758.971,1269.95 760.951,1270.88 762.931,1271.8 764.911,1272.72 766.891,1273.63 768.872,1274.54 770.852,1275.43 772.832,1276.33 774.812,1277.21 776.792,1278.09 \n",
       "  778.772,1278.97 780.753,1279.83 782.733,1280.69 784.713,1281.55 786.693,1282.4 788.673,1283.24 790.653,1284.08 792.634,1284.91 794.614,1285.73 796.594,1286.55 \n",
       "  798.574,1287.36 800.554,1288.17 802.534,1288.97 804.514,1289.76 806.495,1290.55 808.475,1291.34 810.455,1292.11 812.435,1292.88 814.415,1293.65 816.395,1294.41 \n",
       "  818.376,1295.16 820.356,1295.91 822.336,1296.65 824.316,1297.38 826.296,1298.11 828.276,1298.84 830.257,1299.56 832.237,1300.27 834.217,1300.98 836.197,1301.68 \n",
       "  838.177,1302.38 840.157,1303.07 842.138,1303.75 844.118,1304.43 846.098,1305.11 848.078,1305.77 850.058,1306.44 852.038,1307.1 854.019,1307.75 855.999,1308.4 \n",
       "  857.979,1309.04 859.959,1309.68 861.939,1310.31 863.919,1310.93 865.9,1311.56 867.88,1312.17 869.86,1312.78 871.84,1313.39 873.82,1313.99 875.8,1314.59 \n",
       "  877.781,1315.18 879.761,1315.77 881.741,1316.35 883.721,1316.93 885.701,1317.5 887.681,1318.07 889.661,1318.63 891.642,1319.19 893.622,1319.74 895.602,1320.28 \n",
       "  897.582,1320.83 899.562,1321.36 901.542,1321.9 903.523,1322.43 905.503,1322.95 907.483,1323.47 909.463,1323.99 911.443,1324.5 913.423,1325.01 915.404,1325.51 \n",
       "  917.384,1326.01 919.364,1326.51 921.344,1327 923.324,1327.49 925.304,1327.97 927.285,1328.45 929.265,1328.92 931.245,1329.39 933.225,1329.86 935.205,1330.32 \n",
       "  937.185,1330.78 939.166,1331.24 941.146,1331.69 943.126,1332.14 945.106,1332.58 947.086,1333.03 949.066,1333.46 951.047,1333.9 953.027,1334.33 955.007,1334.75 \n",
       "  956.987,1335.18 958.967,1335.6 960.947,1336.01 962.928,1336.43 964.908,1336.84 966.888,1337.24 968.868,1337.64 970.848,1338.04 972.828,1338.44 974.808,1338.83 \n",
       "  976.789,1339.22 978.769,1339.61 980.749,1339.99 982.729,1340.38 984.709,1340.75 986.689,1341.13 988.67,1341.5 990.65,1341.87 992.63,1342.23 994.61,1342.6 \n",
       "  996.59,1342.96 998.57,1343.31 1000.55,1343.67 1002.53,1344.02 1004.51,1344.37 1006.49,1344.71 1008.47,1345.05 1010.45,1345.4 1012.43,1345.73 1014.41,1346.07 \n",
       "  1016.39,1346.4 1018.37,1346.73 1020.35,1347.06 1022.33,1347.38 1024.31,1347.7 1026.29,1348.02 1028.27,1348.34 1030.25,1348.66 1032.23,1348.97 1034.21,1349.28 \n",
       "  1036.19,1349.59 1038.17,1349.89 1040.15,1350.19 1042.13,1350.49 1044.11,1350.79 1046.09,1351.09 1048.07,1351.38 1050.05,1351.67 1052.03,1351.96 1054.02,1352.25 \n",
       "  1056,1352.53 1057.98,1352.82 1059.96,1353.1 1061.94,1353.37 1063.92,1353.65 1065.9,1353.92 1067.88,1354.19 1069.86,1354.46 1071.84,1354.73 1073.82,1354.99 \n",
       "  1075.8,1355.26 1077.78,1355.52 1079.76,1355.78 1081.74,1356.04 1083.72,1356.29 1085.7,1356.55 1087.68,1356.8 1089.66,1357.05 1091.64,1357.3 1093.62,1357.54 \n",
       "  1095.6,1357.79 1097.58,1358.03 1099.56,1358.27 1101.54,1358.51 1103.52,1358.75 1105.5,1358.99 1107.48,1359.22 1109.46,1359.45 1111.44,1359.69 1113.42,1359.91 \n",
       "  1115.4,1360.14 1117.38,1360.37 1119.36,1360.59 1121.34,1360.82 1123.32,1361.04 1125.3,1361.26 1127.28,1361.48 1129.26,1361.7 1131.24,1361.91 1133.22,1362.13 \n",
       "  1135.2,1362.34 1137.18,1362.55 1139.16,1362.76 1141.14,1362.97 1143.12,1363.17 1145.1,1363.38 1147.08,1363.58 1149.06,1363.79 1151.04,1363.99 1153.02,1364.19 \n",
       "  1155,1364.39 1156.98,1364.58 1158.96,1364.78 1160.94,1364.98 1162.92,1365.17 1164.9,1365.36 1166.88,1365.55 1168.86,1365.74 1170.84,1365.93 1172.82,1366.12 \n",
       "  1174.8,1366.3 1176.79,1366.49 1178.77,1366.67 1180.75,1366.85 1182.73,1367.03 1184.71,1367.21 1186.69,1367.39 1188.67,1367.57 1190.65,1367.75 1192.63,1367.92 \n",
       "  1194.61,1368.1 1196.59,1368.27 1198.57,1368.44 1200.55,1368.61 1202.53,1368.78 1204.51,1368.95 1206.49,1369.12 1208.47,1369.29 1210.45,1369.45 1212.43,1369.62 \n",
       "  1214.41,1369.78 1216.39,1369.94 1218.37,1370.1 1220.35,1370.27 1222.33,1370.42 1224.31,1370.58 1226.29,1370.74 1228.27,1370.9 1230.25,1371.05 1232.23,1371.21 \n",
       "  1234.21,1371.36 1236.19,1371.51 1238.17,1371.67 1240.15,1371.82 1242.13,1371.97 1244.11,1372.12 1246.09,1372.26 1248.07,1372.41 1250.05,1372.56 1252.03,1372.7 \n",
       "  1254.01,1372.85 1255.99,1372.99 1257.97,1373.14 1259.95,1373.28 1261.93,1373.42 1263.91,1373.56 1265.89,1373.7 1267.87,1373.84 1269.85,1373.98 1271.83,1374.11 \n",
       "  1273.81,1374.25 1275.79,1374.39 1277.77,1374.52 1279.75,1374.65 1281.73,1374.79 1283.71,1374.92 1285.69,1375.05 1287.67,1375.18 1289.65,1375.31 1291.63,1375.44 \n",
       "  1293.61,1375.57 1295.59,1375.7 1297.58,1375.83 1299.56,1375.95 1301.54,1376.08 1303.52,1376.2 1305.5,1376.33 1307.48,1376.45 1309.46,1376.57 1311.44,1376.7 \n",
       "  1313.42,1376.82 1315.4,1376.94 1317.38,1377.06 1319.36,1377.18 1321.34,1377.3 1323.32,1377.42 1325.3,1377.53 1327.28,1377.65 1329.26,1377.77 1331.24,1377.88 \n",
       "  1333.22,1378 1335.2,1378.11 1337.18,1378.23 1339.16,1378.34 1341.14,1378.45 1343.12,1378.57 1345.1,1378.68 1347.08,1378.79 1349.06,1378.9 1351.04,1379.01 \n",
       "  1353.02,1379.12 1355,1379.22 1356.98,1379.33 1358.96,1379.44 1360.94,1379.55 1362.92,1379.65 1364.9,1379.76 1366.88,1379.86 1368.86,1379.97 1370.84,1380.07 \n",
       "  1372.82,1380.18 1374.8,1380.28 1376.78,1380.38 1378.76,1380.48 1380.74,1380.58 1382.72,1380.68 1384.7,1380.78 1386.68,1380.88 1388.66,1380.98 1390.64,1381.08 \n",
       "  1392.62,1381.18 1394.6,1381.28 1396.58,1381.37 1398.56,1381.47 1400.54,1381.57 1402.52,1381.66 1404.5,1381.76 1406.48,1381.85 1408.46,1381.95 1410.44,1382.04 \n",
       "  1412.42,1382.13 1414.4,1382.23 1416.38,1382.32 1418.36,1382.41 1420.35,1382.5 1422.33,1382.59 1424.31,1382.69 1426.29,1382.78 1428.27,1382.86 1430.25,1382.95 \n",
       "  1432.23,1383.04 1434.21,1383.13 1436.19,1383.22 1438.17,1383.31 1440.15,1383.39 1442.13,1383.48 1444.11,1383.57 1446.09,1383.65 1448.07,1383.74 1450.05,1383.82 \n",
       "  1452.03,1383.91 1454.01,1383.99 1455.99,1384.08 1457.97,1384.16 1459.95,1384.24 1461.93,1384.32 1463.91,1384.41 1465.89,1384.49 1467.87,1384.57 1469.85,1384.65 \n",
       "  1471.83,1384.73 1473.81,1384.81 1475.79,1384.89 1477.77,1384.97 1479.75,1385.05 1481.73,1385.13 1483.71,1385.21 1485.69,1385.28 1487.67,1385.36 1489.65,1385.44 \n",
       "  1491.63,1385.52 1493.61,1385.59 1495.59,1385.67 1497.57,1385.75 1499.55,1385.82 1501.53,1385.9 1503.51,1385.97 1505.49,1386.05 1507.47,1386.12 1509.45,1386.19 \n",
       "  1511.43,1386.27 1513.41,1386.34 1515.39,1386.41 1517.37,1386.49 1519.35,1386.56 1521.33,1386.63 1523.31,1386.7 1525.29,1386.77 1527.27,1386.84 1529.25,1386.91 \n",
       "  1531.23,1386.98 1533.21,1387.05 1535.19,1387.12 1537.17,1387.19 1539.15,1387.26 1541.14,1387.33 1543.12,1387.4 1545.1,1387.47 1547.08,1387.53 1549.06,1387.6 \n",
       "  1551.04,1387.67 1553.02,1387.74 1555,1387.8 1556.98,1387.87 1558.96,1387.94 1560.94,1388 1562.92,1388.07 1564.9,1388.13 1566.88,1388.2 1568.86,1388.26 \n",
       "  1570.84,1388.33 1572.82,1388.39 1574.8,1388.45 1576.78,1388.52 1578.76,1388.58 1580.74,1388.64 1582.72,1388.71 1584.7,1388.77 1586.68,1388.83 1588.66,1388.89 \n",
       "  1590.64,1388.95 1592.62,1389.02 1594.6,1389.08 1596.58,1389.14 1598.56,1389.2 1600.54,1389.26 1602.52,1389.32 1604.5,1389.38 1606.48,1389.44 1608.46,1389.5 \n",
       "  1610.44,1389.56 1612.42,1389.62 1614.4,1389.68 1616.38,1389.73 1618.36,1389.79 1620.34,1389.85 1622.32,1389.91 1624.3,1389.97 1626.28,1390.02 1628.26,1390.08 \n",
       "  1630.24,1390.14 1632.22,1390.19 1634.2,1390.25 1636.18,1390.31 1638.16,1390.36 1640.14,1390.42 1642.12,1390.47 1644.1,1390.53 1646.08,1390.58 1648.06,1390.64 \n",
       "  1650.04,1390.69 1652.02,1390.75 1654,1390.8 1655.98,1390.85 1657.96,1390.91 1659.94,1390.96 1661.93,1391.01 1663.91,1391.07 1665.89,1391.12 1667.87,1391.17 \n",
       "  1669.85,1391.23 1671.83,1391.28 1673.81,1391.33 1675.79,1391.38 1677.77,1391.43 1679.75,1391.49 1681.73,1391.54 1683.71,1391.59 1685.69,1391.64 1687.67,1391.69 \n",
       "  1689.65,1391.74 1691.63,1391.79 1693.61,1391.84 1695.59,1391.89 1697.57,1391.94 1699.55,1391.99 1701.53,1392.04 1703.51,1392.09 1705.49,1392.14 1707.47,1392.18 \n",
       "  1709.45,1392.23 1711.43,1392.28 1713.41,1392.33 1715.39,1392.38 1717.37,1392.43 1719.35,1392.47 1721.33,1392.52 1723.31,1392.57 1725.29,1392.62 1727.27,1392.66 \n",
       "  1729.25,1392.71 1731.23,1392.76 1733.21,1392.8 1735.19,1392.85 1737.17,1392.89 1739.15,1392.94 1741.13,1392.99 1743.11,1393.03 1745.09,1393.08 1747.07,1393.12 \n",
       "  1749.05,1393.17 1751.03,1393.21 1753.01,1393.26 1754.99,1393.3 1756.97,1393.35 1758.95,1393.39 1760.93,1393.43 1762.91,1393.48 1764.89,1393.52 1766.87,1393.57 \n",
       "  1768.85,1393.61 1770.83,1393.65 1772.81,1393.7 1774.79,1393.74 1776.77,1393.78 1778.75,1393.82 1780.73,1393.87 1782.71,1393.91 1784.7,1393.95 1786.68,1393.99 \n",
       "  1788.66,1394.04 1790.64,1394.08 1792.62,1394.12 1794.6,1394.16 1796.58,1394.2 1798.56,1394.24 1800.54,1394.28 1802.52,1394.33 1804.5,1394.37 1806.48,1394.41 \n",
       "  1808.46,1394.45 1810.44,1394.49 1812.42,1394.53 1814.4,1394.57 1816.38,1394.61 1818.36,1394.65 1820.34,1394.69 1822.32,1394.73 1824.3,1394.77 1826.28,1394.81 \n",
       "  1828.26,1394.85 1830.24,1394.88 1832.22,1394.92 1834.2,1394.96 1836.18,1395 1838.16,1395.04 1840.14,1395.08 1842.12,1395.12 1844.1,1395.15 1846.08,1395.19 \n",
       "  1848.06,1395.23 1850.04,1395.27 1852.02,1395.3 1854,1395.34 1855.98,1395.38 1857.96,1395.42 1859.94,1395.45 1861.92,1395.49 1863.9,1395.53 1865.88,1395.56 \n",
       "  1867.86,1395.6 1869.84,1395.64 1871.82,1395.67 1873.8,1395.71 1875.78,1395.75 1877.76,1395.78 1879.74,1395.82 1881.72,1395.85 1883.7,1395.89 1885.68,1395.93 \n",
       "  1887.66,1395.96 1889.64,1396 1891.62,1396.03 1893.6,1396.07 1895.58,1396.1 1897.56,1396.14 1899.54,1396.17 1901.52,1396.21 1903.5,1396.24 1905.49,1396.27 \n",
       "  1907.47,1396.31 1909.45,1396.34 1911.43,1396.38 1913.41,1396.41 1915.39,1396.44 1917.37,1396.48 1919.35,1396.51 1921.33,1396.55 1923.31,1396.58 1925.29,1396.61 \n",
       "  1927.27,1396.64 1929.25,1396.68 1931.23,1396.71 1933.21,1396.74 1935.19,1396.78 1937.17,1396.81 1939.15,1396.84 1941.13,1396.87 1943.11,1396.91 1945.09,1396.94 \n",
       "  1947.07,1396.97 1949.05,1397 1951.03,1397.03 1953.01,1397.07 1954.99,1397.1 1956.97,1397.13 1958.95,1397.16 1960.93,1397.19 1962.91,1397.22 1964.89,1397.26 \n",
       "  1966.87,1397.29 1968.85,1397.32 1970.83,1397.35 1972.81,1397.38 1974.79,1397.41 1976.77,1397.44 1978.75,1397.47 1980.73,1397.5 1982.71,1397.53 1984.69,1397.56 \n",
       "  1986.67,1397.59 1988.65,1397.62 1990.63,1397.65 1992.61,1397.68 1994.59,1397.71 1996.57,1397.74 1998.55,1397.77 2000.53,1397.8 2002.51,1397.83 2004.49,1397.86 \n",
       "  2006.47,1397.89 2008.45,1397.92 2010.43,1397.95 2012.41,1397.98 2014.39,1398.01 2016.37,1398.04 2018.35,1398.06 2020.33,1398.09 2022.31,1398.12 2024.29,1398.15 \n",
       "  2026.27,1398.18 2028.26,1398.21 2030.24,1398.24 2032.22,1398.26 2034.2,1398.29 2036.18,1398.32 2038.16,1398.35 2040.14,1398.38 2042.12,1398.4 2044.1,1398.43 \n",
       "  2046.08,1398.46 2048.06,1398.49 2050.04,1398.51 2052.02,1398.54 2054,1398.57 2055.98,1398.6 2057.96,1398.62 2059.94,1398.65 2061.92,1398.68 2063.9,1398.7 \n",
       "  2065.88,1398.73 2067.86,1398.76 2069.84,1398.78 2071.82,1398.81 2073.8,1398.84 2075.78,1398.86 2077.76,1398.89 2079.74,1398.92 2081.72,1398.94 2083.7,1398.97 \n",
       "  2085.68,1398.99 2087.66,1399.02 2089.64,1399.05 2091.62,1399.07 2093.6,1399.1 2095.58,1399.12 2097.56,1399.15 2099.54,1399.18 2101.52,1399.2 2103.5,1399.23 \n",
       "  2105.48,1399.25 2107.46,1399.28 2109.44,1399.3 2111.42,1399.33 2113.4,1399.35 2115.38,1399.38 2117.36,1399.4 2119.34,1399.43 2121.32,1399.45 2123.3,1399.48 \n",
       "  2125.28,1399.5 2127.26,1399.53 2129.24,1399.55 2131.22,1399.58 2133.2,1399.6 2135.18,1399.62 2137.16,1399.65 2139.14,1399.67 2141.12,1399.7 2143.1,1399.72 \n",
       "  2145.08,1399.74 2147.06,1399.77 2149.05,1399.79 2151.03,1399.82 2153.01,1399.84 2154.99,1399.86 2156.97,1399.89 2158.95,1399.91 2160.93,1399.93 2162.91,1399.96 \n",
       "  2164.89,1399.98 2166.87,1400 2168.85,1400.03 2170.83,1400.05 2172.81,1400.07 2174.79,1400.1 2176.77,1400.12 2178.75,1400.14 2180.73,1400.16 2182.71,1400.19 \n",
       "  2184.69,1400.21 2186.67,1400.23 2188.65,1400.26 2190.63,1400.28 2192.61,1400.3 2194.59,1400.32 2196.57,1400.35 2198.55,1400.37 2200.53,1400.39 2202.51,1400.41 \n",
       "  2204.49,1400.43 2206.47,1400.46 2208.45,1400.48 2210.43,1400.5 2212.41,1400.52 2214.39,1400.54 2216.37,1400.57 2218.35,1400.59 2220.33,1400.61 2222.31,1400.63 \n",
       "  2224.29,1400.65 2226.27,1400.67 2228.25,1400.7 2230.23,1400.72 2232.21,1400.74 2234.19,1400.76 2236.17,1400.78 2238.15,1400.8 2240.13,1400.82 2242.11,1400.84 \n",
       "  2244.09,1400.86 2246.07,1400.89 2248.05,1400.91 2250.03,1400.93 2252.01,1400.95 2253.99,1400.97 2255.97,1400.99 2257.95,1401.01 2259.93,1401.03 2261.91,1401.05 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip3400)\" points=\"\n",
       "1958.43,251.724 2249.26,251.724 2249.26,130.764 1958.43,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1958.43,251.724 2249.26,251.724 2249.26,130.764 1958.43,130.764 1958.43,251.724 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3400)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1982.43,191.244 2126.43,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip3400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2150.43, 208.744)\" x=\"2150.43\" y=\"208.744\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "gr()\n",
    "Plots.plot(losses,xlabel=\"epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = top-level scope at In[65]:1\n",
      "└ @ Core In[65]:1\n",
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = top-level scope at In[65]:1\n",
      "└ @ Core In[65]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.14285714285714"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=argmax(knetpredict(wmlp,train_input)) .== argmax(train_output)\n",
    "sum(res)/length(res)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = top-level scope at In[66]:1\n",
      "└ @ Core In[66]:1\n",
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = top-level scope at In[66]:1\n",
      "└ @ Core In[66]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.55555555555556"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=argmax(knetpredict(wmlp,test_input)) .== argmax(test_output)\n",
    "k_accuracy = sum(res)/length(res)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
